{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75080caf",
   "metadata": {},
   "source": [
    "# Session 1\n",
    "\n",
    "Let's get hands-on! During today's exercise, we will start from the basics: \n",
    "- Learn how to access data files from the ATLAS Open Data website.\n",
    "- Get familiar with some analysis tools: $\\texttt{python}$ in $\\texttt{jupyter notebook}$, $\\texttt{numpy}$ (this session) and finally ROOT (next session).\n",
    "- Explore the data with $\\texttt{numpy}$ by making histograms and constructing particle Lorentz vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387ba2c8",
   "metadata": {},
   "source": [
    "## ATLAS Open Data\n",
    "\n",
    "[ATLAS Open Data](http://opendata.atlas.cern/) is an open dataset of both simulated and real proton-proton collision events. \n",
    "We will be using the [ATLAS 13 TeV Open Dataset](https://opendata.atlas.cern/docs/documentation/overview_data/data_education_2020). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f322c52c",
   "metadata": {},
   "source": [
    "## Analysis Tools\n",
    "\n",
    "\n",
    "In today's session we will be using $\\texttt{uproot}$ and $\\texttt{numpy}$ to access events in ATLAS Open Data data files. Uproot is a python serialisation library that converts files from a ROOT format (a data format ubiqitiously used in High Energy Physics analysis) to regular arrays so we can manipulate it using regular but powerful python libraries. Specifically, we will be \"uprooting\" the data to arrays in $\\texttt{numpy}$, a python library for multidimensional array and matrix manipulation.\n",
    "\n",
    "We will start by analysing the particles from a simulation of the following process and one of the main Standard Model processes that occur at the LHC, $t\\bar{t}$ production and decay to leptons, neutrinos and b-quarks:\n",
    "<img src=\"https://cds.cern.ch/record/2299951/files/feynman_ttbar_emu.png\" width=\"500\" />\n",
    "\n",
    "In your dataset, the muons ($\\mu^-/\\mu^+$) and electrons ($e^-$\\$e^+$) are together described as leptons (e.g. \"lep_E\" are the lepton energies). They can be distinguished as muon/electron by querying the \"lep_type\" vairable (11 for electrons, 13 for muons). The b-quarks are reconstructed as jets in your dataset (e.g. \"jet_E\" are the jet energies). But remember, quark and gluon interactions in proton collisions also produce so-called initial state and final state radiation, mostly in the form of more jets, and in addition, these datasets have on the order of $\\mathcal{O}(10s)$ of proton-proton interactions in the same event - these are almost always uninteresting low energy quantumchromodynamic processes resulting in additional jets!\n",
    "Finally, neutrinos are invisible to the detector and can only be identified as total missing energy (\"met\").\n",
    "\n",
    "> **Some important terminology**\n",
    "\n",
    "> ... Often we are interested in only the most energetic particles, for which we have terms for:\n",
    "> - \"leading\" particle: The particle (lepton, quark) with the highest measured transverse momentum.\n",
    "> - \"subleading\" particle: The particle (lepton, quark) with the *second* highest measured transverse momentum.\n",
    "(we similarly get \"subsubleading\", and \"subsubsubleading\"...)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9800e5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install aiohttp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331ebbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first import the necessary libraries and define our global variables\n",
    "import uproot\n",
    "import numpy as np\n",
    "\n",
    "base_url = 'https://atlas-opendata.web.cern.ch/atlas-opendata/samples/2020/2lep/MC/'\n",
    "input_file =  'mc_410000.ttbar_lep.2lep.root'\n",
    "tree_name = 'mini' # event \"tree\" in which information of each event in the data set are defined:\n",
    "                   # event level information, particles and their properties\n",
    "\n",
    "# function to retrieve the data from an input file.\n",
    "def get_events(base_url,input_file,tree_name):\n",
    "    events = uproot.open(f\"{base_url}/{input_file}:{tree_name}\")\n",
    "    return events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dab0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we read in the content file and print the file content\n",
    "\n",
    "events = get_events(base_url,input_file,tree_name)\n",
    "events.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7ab123",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# now lets convert the variables we want to analyse into numpy arrays. THIS MAY TAKE A FEW SECONDS!\n",
    "# The resulting 'lep_kinematics' variable is known as a structured numpy array.\n",
    "\n",
    "max_events = 10 # just sampling some events\n",
    "\n",
    "lep_kinematics = events.arrays(['lep_n', 'lep_pt', 'lep_eta', 'lep_phi', 'lep_E'], \n",
    "                               library=\"np\", \n",
    "                               entry_stop=max_events)                    \n",
    "\n",
    "print(\"Number of leptons in first 5 events:\\n\", lep_kinematics['lep_n'][:5])\n",
    "print('particle kinematics of leading lepton in first 5 events:')\n",
    "for i in range(5):\n",
    "    if lep_kinematics['lep_n'][i] >=1:\n",
    "        print('lepton pT: %i MeV, eta: %f.2, phi: %f.2.'%(lep_kinematics['lep_pt'][i][0],lep_kinematics['lep_eta'][i][0], lep_kinematics['lep_phi'][i][0]))\n",
    "    else:\n",
    "        print(\"WARNING No leptons found in this event (number of leptons = %i)\"%(lep_kinematics['lep_n'][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3ab580",
   "metadata": {},
   "source": [
    "### Histograms\n",
    "\n",
    "Now that you know how to retrieve and access the data content, you are ready to analyse it.\n",
    "Histograms are a good way to visualise a dataset and are fundamental to data analysis. They are a graphical representation of the distribution of a dataset; A way to visualize the frequency distribution of a set of continuous or discrete data.\n",
    "In particle physics, histograms are often used to visualize the distribution of particle properties such as energy, momentum, or mass. A histogram is created by dividing a variable range into a set of \"bins\" and counting the number of times a particle or event property falls in that bin. Thus, the height of a histogram represents the frequency of events that fall into that bin.\n",
    "\n",
    "Histograms can be very useful for understanding features in your data. Here, for example is the distribution of the size of all exo-planets in NASA's Kepler space mission database, showing that smaller planets are more common than massive planets, and that there is an interesting multi-peak feature:\n",
    "\n",
    "<img src=\"https://www.nasa.gov/wp-content/uploads/2017/06/press-web19_small_planets_two_sizes-edit.jpg\" width=\"500\" />\n",
    "\n",
    "\n",
    "(ref: [NASA Kepler website](https://www.nasa.gov/ames/kepler/briefing-materials-final-kepler-survey-catalog-of-planet-candidates-in-the-cygnus-field))\n",
    "\n",
    "A histogram can also be normalised which changes the definition of its content. For example, in the Kepler histograms above each bin measures the number of planets per 100 stars, which is in effect a rate measurement: There are roughly 6 Earth-sized planets for every 100 stars.\n",
    "If one were to sum the counts (all bin heights) and divide all bin counts by the total sum, one would get a rough probablity density distribution: The fraction or probability of planets of a certain radius."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10af3dbd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Making a histogram\n",
    "\n",
    "Let's make a histogram of some particle variables. A histogram is defined by two arrays: the bin edges and the number of entries in each bin. \n",
    "\n",
    "\n",
    "\n",
    "> **Side note: Underflows and overflows** When defining a histogram you need to choose the total range over which the data will be represented as you will be choosing the first bin edge and the last bin edge. Any data that falls outside of this range can be filled in the so-called \"underflow\" and \"overflow\" bins. As the under- and overflow bins theoretically don't have a lower and and upper edge, respectively, they will not accurately represent the continued data distribution. Thus they are not commonly plotted as part of the histogram, however it is sometimes useful to know the fraction of data your histogram is representing relative to the fraction outside its range.\n",
    "\n",
    "#### Variables for histogramming\n",
    "\n",
    "Below you will retrieve a variable from your input file and make histogram distributions - we'll start by retrieving the number of jets per event (\"jet_n\").\n",
    "\n",
    "After that you can try plotting other variables. Refer to the print out in the second cell of the notebook or see the [ATLAS Open Data Website here](http://opendata.atlas.cern/release/2020/documentation/datasets/dataset13.html) for a list of variables in the dataset. Ask us if you don't understand what a variable means! We will also gradually be learning about most of these variables during this course. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833ab38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, retrieve the data you want. \n",
    "\n",
    "# Note: The ttbar sample is a large sample: You might want to limit the events to analyse.\n",
    "max_events = 500000 \n",
    "\n",
    "list_of_variables = ['jet_n']  # <-- ADD YOUR VARIABLES HERE\n",
    "\n",
    "print(f\"INFO: retrieving {list_of_variables} for the first {max_events} events.\\n\")\n",
    "particle_arrays = events.arrays(list_of_variables, library=\"np\", entry_stop=max_events) \n",
    "\n",
    "# You may want to print out your variables of interest for a subset of events to get an idea of their magnitude:\n",
    "# ( Or better yet print the maximum and minimum value in the array - this is easy for event variables. )\n",
    "for key in particle_arrays:\n",
    "    print(key, \" for first 5 events:\\n\", particle_arrays[key][:5])\n",
    "    if not isinstance(particle_arrays[key][0], np.ndarray):\n",
    "        #print(\"This variable is an event variable\")\n",
    "        print(f\"min {key}: \", particle_arrays[key].min())\n",
    "        print(f\"max {key}: \", particle_arrays[key].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a32b3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next, define your histogram in numpy\n",
    "# Remember, a histogram consists of 2 arrays: The bin edges, and the bin counts.\n",
    "# The bin edges are defined by the minimum and maximum value of the variable range, and the bin width \n",
    "# (the value range that each bin represents).\n",
    "# While histograms do *not* have to have bins of equal width, lets choose one bin size.\n",
    "\n",
    "var_key = 'jet_n' # FILL ME with chosen variable to histogram\n",
    "\n",
    "# ADJUST these parameters to make it suitable to the variable you are looking at.\n",
    "bin_width = 99\n",
    "hist_min = 0\n",
    "hist_max = 99\n",
    "bins = int((hist_max-hist_min)/bin_width)\n",
    "\n",
    "\n",
    "# define histogram using numpy.histogram\n",
    "bin_entries, bin_edges = np.histogram(particle_arrays[var_key][0]+particle_arrays[var_key][1], bins=bins, range=[hist_min,hist_max])\n",
    "\n",
    "print('bin edges: ', bin_edges)\n",
    "print('number of bin edges: ', len(bin_edges))\n",
    "print('number of bin entries: ', len(bin_entries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f69a4f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now we DRAW the histogram\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# bin_edges[:-1] to ignore last bin edge.\n",
    "plt.bar(bin_edges[:-1], bin_entries, width=bin_width, label = f'{var_key}', color='blue', alpha=0.5) \n",
    "legend = plt.legend(loc=\"upper right\")\n",
    "\n",
    "# set appropriate x and y axes labels : think about it!\n",
    "xlabel = 'x label'\n",
    "ylabel = 'y label'\n",
    "\n",
    "plt.xlabel(xlabel)\n",
    "plt.ylabel(ylabel)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453de75a",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "- How would you label the x-axis and y-axis of your histogram?\n",
    "- How would you modify your \"number of jets\" histogram distribution if you wanted to measure the _probability density distribution_ (e.g. if you wanted to know the probability of an event containing extactly 2 jets)?\n",
    "- What histograms would you want to plot if you wanted to know more about these $t\\bar{t}$ leptonic decay events, specifically:\n",
    "    - How many leptons does ATLAS reconstruct for these events? Does it look different to the number of jets, and why?\n",
    "    - How much missing transverse energy do $t\\bar{t}$ decays typically produce?\n",
    "    - Do the two most energetic leptons have opposite charge?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757f788f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exploring data on an event display\n",
    "\n",
    "Next we will explore the data in a different way: making event displays of collision events! Event displays is the reconstructed image of a single event. To make an event display we want to \"reconstruct\" the particles first. \n",
    "\n",
    "### Particle four-momenta\n",
    "We describe particles by their type and their four-momentum, which is a four-component vector consisting of their energy and momentum in the three spatial dimensions. This four-momentum vector is known as a Lorentz vector. The components of a Lorentz vector depend on the frame of reference (in our case it is the lab reference frame), and they transform in a specific way under Lorentz transformations, which are the mathematical transformations that relate the measurements made in different frames of reference.\n",
    "\n",
    "### Transformation to cartesian coordinates\n",
    "The momentum and direction of particles in the data file are described by $p_T$ (transverse momemtum), $\\eta$ and $\\phi$ (polar coordinates). Transforming these to cartesian coordinates $p_x$, $p_y$, $p_z$, just requires some basic trigonometry. Refer to the description of the ATLAS coordinate system in [today's intro session](https://github.com/pkontaxa/OpenData_Unige/blob/main/Session1/Session1_Intro.ipynb), and note that $\\theta = 2 arctan(e^{-\\eta})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e8982d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Here we create some classes and functions to reconstruct our particles.\n",
    "# the four momentum class reads in the particle properties contained in the data samples and\n",
    "# transforms these \n",
    "\n",
    "import math\n",
    "\n",
    "class four_momentum:\n",
    "    def __init__(self, pt, eta, phi, E):\n",
    "        self.pt=pt\n",
    "        self.eta=eta\n",
    "        self.phi=phi\n",
    "        self.E=E\n",
    "        self.theta = 0 # FILL ME # hint: use math.atan, math.e to convert eta -> theta\n",
    "        self.px = 0 # FILL ME\n",
    "        self.py = 0 # FILL ME\n",
    "        self.pz = 0 # FILL ME\n",
    "\n",
    "    def get_ptetaphi(self,px,py,pz):\n",
    "        pt =  0 # FILL ME (Homework CHALLENGE)\n",
    "        eta = 0 # FILL ME (Homework CHALLENGE)\n",
    "        phi = 0 # FILL ME (Homework CHALLENGE)\n",
    "        return pt,eta,phi\n",
    "    def mass(self):\n",
    "        print(\"fill me\")\n",
    "        return 0 # FILL ME (Homework CHALLENGE)\n",
    "    def __add__(self, fourvec2):  # overriding the \"+\" operator\n",
    "        print(\"fill me: add another 4-momentum vector to me \")\n",
    "        E_new = self.E+ fourvec2.E\n",
    "        px_new = 0 # FILL ME (Homework CHALLENGE)\n",
    "        py_new = 0 # FILL ME (Homework CHALLENGE)\n",
    "        pz_new = 0 # FILL ME (Homework CHALLENGE)\n",
    "        pt_new,eta_new,phi_new = self.get_ptetaphi(px_new,py_new,pz_new)\n",
    "        return four_momentum(pt_new,eta_new,phi_new,E_new) \n",
    "        \n",
    "def print4vec(fourvec):\n",
    "    print(\"E  : %.2f MeV\"%(fourvec.E))\n",
    "    print(\"pt : %.2f MeV\"%(fourvec.pt))\n",
    "    print(\"eta: %.2f    phi: %.2f\"%(fourvec.eta,fourvec.phi))\n",
    "    print(\" px: %.2f MeV py: %.2f MeV\\n\"%(fourvec.px,fourvec.py))\n",
    "    \n",
    "def get_four_momentum(ptype, pIdx, evt, data):\n",
    "    'Function to construct the 4-momentum of a particle'\n",
    "    if not ptype in [\"lep\", \"jet\", \"photon\", \"largeRjet\", \"tau\"]:\n",
    "        raise ValueError(\"particle type \", ptype, \"not recognised\")\n",
    "    prefix = f\"{ptype}_\"\n",
    "    E   = data[prefix+\"E\"][evt][pIdx]\n",
    "    pt  = data[prefix+\"pt\"][evt][pIdx]\n",
    "    eta = data[prefix+\"eta\"][evt][pIdx]\n",
    "    phi = data[prefix+\"phi\"][evt][pIdx]\n",
    "    return four_momentum(pt,eta,phi,E)\n",
    "\n",
    "map_lep_type = {11: \"electron/positron\", 13:\"muon\"}  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07a9d4b",
   "metadata": {},
   "source": [
    "### Event process\n",
    "\n",
    "This time we will be analysing di-boson production and decay, specifically the production of a Z-boson and $W^-$/$W^+$ boson. These subsequently decay to two leptons: $Z\\rightarrow\\ell^+\\ell^-$; and two quarks $W^+\\rightarrow q\\bar{q}$ (which of course immediately hadronize and are reconstructed as jets).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3342a1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'mc_363358.WqqZll.2lep.root' # 'mc_410000.ttbar_lep.2lep.root'\n",
    "\n",
    "# function to retrieve the data from an input file.\n",
    "events = get_events(base_url,input_file,tree_name)\n",
    "events.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a166aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Lets reconstruct particles and make an event display. \n",
    "# Lets start by retrieving all variables needed to reconstruct the LEADING and SUBLEADING lepton\n",
    "max_events = 5000\n",
    "\n",
    "list_of_variables = ['lep_E', 'lep_pt', 'lep_eta', 'lep_phi', 'lep_type']  # <-- ADD YOUR VARIABLES HERE\n",
    "lep_variables = events.arrays(list_of_variables, library=\"np\", entry_stop=max_events) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1863e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we finally build our particles and make the event displays\n",
    "\n",
    "# some cosmetic settings\n",
    "colours = {\"muon\": \"blue\", \"electron/positron\": \"orange\", \"jet\":\"red\"}\n",
    "width_scale = 1e-7\n",
    "arrow_scale = 4\n",
    "\n",
    "# Which event do we want to look at?\n",
    "event_no = 9\n",
    "\n",
    "# Retrieve our particles\n",
    "leading_lep_type = lep_variables[\"lep_type\"][event_no][0]\n",
    "leading_lep = get_four_momentum(\"lep\", 0, event_no, lep_variables) # 0 for leading lepton\n",
    "subleading_lep_type = lep_variables[\"lep_type\"][event_no][1] \n",
    "subleading_lep = get_four_momentum(\"lep\", 1, event_no, lep_variables) # 1 for subleading lepton\n",
    "\n",
    "print(\"leading lepton properties: \")\n",
    "print(\"type: \", map_lep_type[leading_lep_type] )\n",
    "print4vec(leading_lep)\n",
    "print(\"subleading lepton properties: \")\n",
    "print(\"type: \", map_lep_type[subleading_lep_type] )\n",
    "print4vec(subleading_lep)\n",
    "\n",
    "leading_lep_colour = colours[map_lep_type[leading_lep_type]]\n",
    "subleading_lep_colour = colours[map_lep_type[subleading_lep_type]]\n",
    "\n",
    "\n",
    "# Draw ATLAS detector structure\n",
    "radii = [1, 2, 4, 12] # Inner Detector, EM calorimeter, HAD calorimeter, muon spectrometer\n",
    "colours = [\"cyan\", \"green\", \"red\", \"grey\"]\n",
    "def circle(r,phi):\n",
    "    return r*np.cos(phi), r*np.sin(phi)\n",
    "\n",
    "plt.axis('equal')\n",
    "phis = np.arange(0,6.28,0.01)\n",
    "for idx,r in enumerate(radii):\n",
    "    plt.plot(*circle(r,phis),color=colours[idx], alpha=0.5, ls='-' )\n",
    "\n",
    "## Draw particles\n",
    "\n",
    "## [0,0],[0,0] refers to the coordinates of \"origin\". [0,0], [0,0] is a good approximation\n",
    "## for the production point of a particle: The two beams are squeezed into a cross-sectional area \n",
    "## that is only micrometers in diameter. Meanwhile the beam pipe is ~50 mm in diameter! \n",
    "## Most particles will already have decayed before leaving the beampipe.\n",
    "plt.quiver([0,0],[0,0], [0,leading_lep.px],[0,leading_lep.py], \n",
    "            width=width_scale*leading_lep.E,\n",
    "            scale=arrow_scale*leading_lep.pt, \n",
    "             color=leading_lep_colour)\n",
    "plt.quiver([0,0],[0,0], [0,subleading_lep.py],[0,subleading_lep.py], \n",
    "            width=width_scale*subleading_lep.E,\n",
    "            scale=arrow_scale*subleading_lep.pt, \n",
    "            color=subleading_lep_colour)\n",
    "\n",
    "plt.xlabel(\"x [arbitrary units]\")\n",
    "plt.ylabel(\"y [arbitrary units]\")\n",
    "#bounds=15\n",
    "#plt.xlim(-1*bounds,1*bounds)\n",
    "#plt.ylim(-1*bounds,1*bounds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065be6aa",
   "metadata": {},
   "source": [
    "### Explore the event display\n",
    "\n",
    "Explore the event display by adding more information. For example, there is also a W-boson decay in each event: Can you reconstruct its decay products and visualise them?\n",
    "\n",
    "> Side note: Sometimes the data does not contain all the reconstructed objects you expect to find as they might have shot out outside the detector acceptance region, and sometimes you find different reconstructed objects from background processes. Remember, these datasets have on the order of $\\mathcal{O}(10s)$ of proton-proton interactions in the same event - these are almost always uninteresting low energy quantumchromodynamic processes resulting in additional jets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1faa6f",
   "metadata": {},
   "source": [
    "### Homework: Reconstruct the di-lepton mass and analyse it\n",
    "\n",
    "Use the above `class four_momentum` to add two four-momenta together to reconstruct a parent particle. Then write a function for the class that computes the invarient mass of a particle defined as a 4-vector.\n",
    "\n",
    "Consider the $Z\\rightarrow\\ell\\ell$,$W\\rightarrow q\\bar{q}$ sample: What two types of particle objects would you add together and reconstruct the combined mass for? At what mass do you expect them to resonate?\n",
    "\n",
    "For this exercise, you can both experiment with adding the combined 4-vector to the event display and making a histogram of the mass of the combined 4-vector. \n",
    "\n",
    "### Bonus questions\n",
    "- If we were to transform our system into the parent particle's rest frame (parent particle has 0 momentum in its rest frame):\n",
    "  - How would the parent particle's and the decay product momentum vectors change?\n",
    "  - How would the reconstructed invariant mass of the di-lepton or di-jet system change?\n",
    "- How do the physical quantities we've discussed today change in a $e^+$-$e^-$ collider?\n"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
